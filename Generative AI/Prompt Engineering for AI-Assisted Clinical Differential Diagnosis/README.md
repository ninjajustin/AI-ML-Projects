# Prompt Engineering for AI-Assisted Clinical Differential Diagnosis
In this assignment, I explored how different prompt engineering strategies affect the output of large language models when generating clinical differential diagnoses. I implemented and tested at least three prompt types—including zero-shot, few-shot, chain-of-thought, and critique-and-refine—using a dataset of patient cases. I evaluated each prompt’s effectiveness based on output quality, accuracy, and consistency with known diagnoses. Finally, I reflected on best practices, how prompt engineering can combine with structured retrieval to reduce errors, and the risks posed by poorly designed prompts in clinical settings.