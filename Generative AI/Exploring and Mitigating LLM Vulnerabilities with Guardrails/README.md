# Exploring and Mitigating LLM Vulnerabilities with Guardrails
In this assignment, I studied common vulnerabilities in large language models (LLMs) such as prompt injection and toxic content generation. By running specific inputs in a provided Python notebook, I observed how unguarded models can be manipulated to produce unsafe or harmful outputs. I then applied different mitigation strategies, including input filtering, prompt rewriting, and response moderation, to reduce these risks. Testing multiple approaches helped me understand their strengths and limitations, especially the tradeoff between blocking harmful content and preserving useful model responses. Finally, I reflected on ethical considerations, emphasizing the importance of transparency, user consent, and human oversight when deploying LLMs in real-world settings.